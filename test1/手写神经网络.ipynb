{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三层神经网络代码\n",
    "\n",
    "### 激活函数和恒等函数\n",
    "- **identify_function**: 恒等函数，直接返回输入值。\n",
    "- **sigmoid**: Sigmoid激活函数，用于将输入值压缩到0和1之间，常用于神经网络的隐藏层。\n",
    "- **softmax**: Softmax函数，用于计算多分类问题中每个类别的概率，通常用于神经网络的输出层。\n",
    "\n",
    "### 损失函数\n",
    "- **cross_entropy_error**: 交叉熵损失函数，用于计算预测值和真实值之间的误差，常用于分类问题的损失计算。\n",
    "\n",
    "### 初始化网络\n",
    "- **init_network**: 初始化神经网络的权重和偏置，返回一个包含这些参数的字典。这里使用的是给定的固定参数。\n",
    "\n",
    "### 前向传播\n",
    "- **forward**: 前向传播函数，依次计算每一层的加权和及激活函数，最终输出经过softmax处理的结果，同时返回隐藏层的输出。\n",
    "\n",
    "### 反向传播\n",
    "- **backward**: 反向传播函数，计算损失函数相对于每个参数的梯度，并使用梯度下降法更新网络的权重和偏置。\n",
    "\n",
    "### 预测\n",
    "- **predict**: 预测函数，使用训练后的网络进行预测，返回网络的输出结果。\n",
    "\n",
    "### 训练过程\n",
    "- **训练过程**: 在训练过程中，输入数据和真实标签通过前向传播计算输出，然后通过反向传播计算梯度并更新网络的参数。训练多个周期（epochs），不断优化网络参数以减少损失函数值。\n",
    "\n",
    "### 预测过程\n",
    "- **预测过程**: 使用训练后的网络对新输入数据进行预测，输出分类结果的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40625907 0.59374093]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def identify_function(x):\n",
    "    \"\"\"恒等函数，直接返回输入值\"\"\"\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid激活函数\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"Softmax函数\"\"\"\n",
    "    c = np.max(a)  # 防止溢出\n",
    "    exp_a = np.exp(a - c)  # 减去最大值c\n",
    "    sum_exp_a = np.sum(exp_a)  # 指数函数的和\n",
    "    y = exp_a / sum_exp_a  # 归一化\n",
    "    return y\n",
    "\n",
    "def init_network():\n",
    "    \"\"\"初始化神经网络，返回一个包含权重和偏置的字典\"\"\"\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    \"\"\"前向传播函数，计算并返回输出\"\"\"\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    # 第一层\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    \n",
    "    # 第二层\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    \n",
    "    # 第三层\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# 初始化网络\n",
    "network = init_network()\n",
    "\n",
    "# 输入数据\n",
    "x = np.array([1.0, 0.5])\n",
    "\n",
    "# 前向传播，计算输出\n",
    "y = forward(network, x)\n",
    "\n",
    "# 打印输出\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三层神经网络代码（训练）\n",
    "\n",
    "### 激活函数和恒等函数\n",
    "- **identify_function**: 恒等函数，直接返回输入值。\n",
    "- **sigmoid**: Sigmoid激活函数，用于将输入值压缩到0和1之间，常用于神经网络的隐藏层。\n",
    "- **softmax**: Softmax函数，用于计算多分类问题中每个类别的概率，通常用于神经网络的输出层。\n",
    "\n",
    "### 损失函数\n",
    "- **cross_entropy_error**: 交叉熵损失函数，用于计算预测值和真实值之间的误差，常用于分类问题的损失计算。\n",
    "\n",
    "### 初始化网络\n",
    "- **init_network**: 初始化神经网络的权重和偏置，返回一个包含这些参数的字典。与之前的版本不同，这里使用随机数进行初始化，而不是固定参数。\n",
    "\n",
    "### 前向传播\n",
    "- **forward**: 前向传播函数，依次计算每一层的加权和及激活函数，最终输出经过softmax处理的结果，同时返回隐藏层的输出。\n",
    "\n",
    "### 反向传播\n",
    "- **backward**: 反向传播函数，计算损失函数相对于每个参数的梯度，并使用梯度下降法更新网络的权重和偏置。\n",
    "\n",
    "### 预测\n",
    "- **predict**: 预测函数，使用训练后的网络进行预测，返回网络的输出结果。\n",
    "\n",
    "### 训练过程\n",
    "- **训练过程**: 在训练过程中，输入数据和真实标签通过前向传播计算输出，然后通过反向传播计算梯度并更新网络的参数。训练多个周期（epochs），不断优化网络参数以减少损失函数值。\n",
    "\n",
    "### 预测过程\n",
    "- **预测过程**: 使用训练后的网络对新输入数据进行预测，输出分类结果的概率分布。\n",
    "\n",
    "### 区别总结\n",
    "1. **初始化网络**: 原始版本使用固定参数进行初始化，修改版本使用随机数进行初始化。\n",
    "2. **反向传播**: 两个版本都实现了反向传播和参数更新，但由于初始化参数的不同，训练的具体过程和结果会有所不同。\n",
    "3. **整体框架**: 其余部分基本一致，都包括前向传播、反向传播、训练和预测功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def identify_function(x):\n",
    "    \"\"\"恒等函数，直接返回输入值\"\"\"\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid激活函数\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"Softmax函数\"\"\"\n",
    "    c = np.max(a)  # 防止溢出\n",
    "    exp_a = np.exp(a - c)  # 减去最大值c\n",
    "    sum_exp_a = np.sum(exp_a)  # 指数函数的和\n",
    "    y = exp_a / sum_exp_a  # 归一化\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"交叉熵损失函数\"\"\"\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "def init_network():\n",
    "    \"\"\"初始化神经网络，返回一个包含权重和偏置的字典\"\"\"\n",
    "    network = {}\n",
    "    # 随机初始化权重和偏置，打破对称性\n",
    "    network['W1'] = np.random.randn(2, 3)\n",
    "    network['b1'] = np.random.randn(3)\n",
    "    network['W2'] = np.random.randn(3, 2)\n",
    "    network['b2'] = np.random.randn(2)\n",
    "    network['W3'] = np.random.randn(2, 2)\n",
    "    network['b3'] = np.random.randn(2)\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    \"\"\"前向传播函数，计算并返回输出\"\"\"\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    # 第一层\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    \n",
    "    # 第二层\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    \n",
    "    # 第三层\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y, z1, z2\n",
    "\n",
    "def backward(network, x, t, y, z1, z2, learning_rate=0.1):\n",
    "    \"\"\"反向传播并更新参数\"\"\"\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    # 输出层梯度\n",
    "    dy = (y - t) / y.shape[0]\n",
    "    \n",
    "    # 第二层到输出层梯度\n",
    "    dW3 = np.dot(z2.T, dy)\n",
    "    db3 = np.sum(dy, axis=0)\n",
    "    \n",
    "    # 第一层到第二层梯度\n",
    "    dz2 = np.dot(dy, W3.T)\n",
    "    da2 = dz2 * z2 * (1 - z2)\n",
    "    dW2 = np.dot(z1.T, da2)\n",
    "    db2 = np.sum(da2, axis=0)\n",
    "    \n",
    "    # 输入层到第一层梯度\n",
    "    dz1 = np.dot(da2, W2.T)\n",
    "    da1 = dz1 * z1 * (1 - z1)\n",
    "    dW1 = np.dot(x.T, da1)\n",
    "    db1 = np.sum(da1, axis=0)\n",
    "    \n",
    "    # 参数更新\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    # 更新网络参数\n",
    "    network['W1'], network['b1'] = W1, b1\n",
    "    network['W2'], network['b2'] = W2, b2\n",
    "    network['W3'], network['b3'] = W3, b3\n",
    "\n",
    "def predict(network, x):\n",
    "    \"\"\"预测函数\"\"\"\n",
    "    y, _, _ = forward(network, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4266695925286395\n",
      "Epoch 100, Loss: 0.027504099593979658\n",
      "Epoch 200, Loss: 0.013537525897145922\n",
      "Epoch 300, Loss: 0.008884842748964068\n",
      "Epoch 400, Loss: 0.006578601023122143\n",
      "Epoch 500, Loss: 0.005206846678952185\n",
      "Epoch 600, Loss: 0.004299545823620574\n",
      "Epoch 700, Loss: 0.0036560562738302957\n",
      "Epoch 800, Loss: 0.003176514757953373\n",
      "Epoch 900, Loss: 0.002805695754796536\n",
      "Predicted: [[0.00250756 0.99749244]]\n"
     ]
    }
   ],
   "source": [
    "# 尝试训练并预测\n",
    "# 初始化网络\n",
    "network = init_network()\n",
    "\n",
    "# 输入数据\n",
    "x = np.array([[1.0, 0.5]])\n",
    "t = np.array([[0, 1]])  # 真实标签，使用one-hot编码\n",
    "\n",
    "# 训练过程\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    y, z1, z2 = forward(network, x)\n",
    "    loss = cross_entropy_error(y, t)\n",
    "    backward(network, x, t, y, z1, z2)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# 预测\n",
    "x_test = np.array([[1.0, 0.5]])\n",
    "y_test = predict(network, x_test)\n",
    "print(\"Predicted:\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "葱苓sama🅥⁧喵⁧‭"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
